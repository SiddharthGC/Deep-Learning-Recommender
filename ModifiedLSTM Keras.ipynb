{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    User  MovieID  a1  a2  a3  a4  a5  a6  a7  a8  ...   a10  a11  a12  a13  \\\n",
      "0      1     1193   0   0   0   0   0   0   0   1  ...     0    0    0    0   \n",
      "1      1      661   0   0   0   0   1   0   0   0  ...     0    1    0    1   \n",
      "2      1      914   0   0   0   0   0   0   0   0  ...     0    1    0    0   \n",
      "3      1     3408   0   0   0   0   0   0   0   1  ...     0    0    0    0   \n",
      "4      1     2355   0   0   1   0   1   0   0   0  ...     0    0    0    1   \n",
      "5      1     1197   1   1   1   0   0   0   0   0  ...     0    0    0    0   \n",
      "6      1     1287   1   1   0   0   0   0   0   1  ...     0    0    0    0   \n",
      "7      1     2804   0   0   1   0   0   0   0   1  ...     0    0    0    0   \n",
      "8      1      594   0   0   0   0   1   0   0   0  ...     0    1    0    1   \n",
      "9      1      919   0   1   0   0   1   0   0   1  ...     0    1    0    0   \n",
      "10     1      595   0   0   0   0   1   0   0   0  ...     0    1    0    1   \n",
      "11     1      938   0   0   0   0   0   0   0   0  ...     0    1    0    0   \n",
      "12     1     2398   0   0   0   0   0   0   0   1  ...     0    0    0    0   \n",
      "13     1     2918   0   0   1   0   0   0   0   0  ...     0    0    0    0   \n",
      "14     1     1035   0   0   0   0   0   0   0   0  ...     0    1    0    0   \n",
      "15     1     2791   0   0   1   0   0   0   0   0  ...     0    0    0    0   \n",
      "16     1     2687   0   0   0   0   1   0   0   0  ...     0    0    0    1   \n",
      "17     1     2018   0   0   0   0   1   0   0   0  ...     0    0    0    1   \n",
      "18     1     3105   0   0   0   0   0   0   0   1  ...     0    0    0    0   \n",
      "19     1     2797   0   0   1   0   0   0   0   0  ...     0    0    0    0   \n",
      "20     1     2321   0   0   1   0   0   0   0   0  ...     0    0    0    0   \n",
      "21     1      720   0   0   0   0   0   0   0   0  ...     0    0    0    1   \n",
      "22     1     1270   0   0   1   0   0   0   0   0  ...     0    0    0    0   \n",
      "23     1      527   0   0   0   0   0   0   0   1  ...     0    0    0    0   \n",
      "24     1     2340   0   0   0   0   0   0   0   0  ...     0    0    0    0   \n",
      "25     1       48   0   0   0   0   1   0   0   0  ...     0    1    0    1   \n",
      "26     1     1097   0   0   0   0   1   0   0   1  ...     0    0    0    0   \n",
      "27     1     1721   0   0   0   0   0   0   0   1  ...     0    0    0    0   \n",
      "28     1     1545   0   0   0   0   0   0   0   1  ...     0    0    0    0   \n",
      "29     1      745   0   0   1   0   0   0   0   0  ...     0    0    0    1   \n",
      "30     1     2294   0   0   0   0   1   0   0   0  ...     0    0    0    1   \n",
      "31     1     3186   0   0   0   0   0   0   0   1  ...     0    0    0    0   \n",
      "32     1     1566   0   1   1   0   1   0   0   0  ...     0    1    0    1   \n",
      "33     1      588   0   0   1   0   1   0   0   0  ...     0    1    0    1   \n",
      "34     1     1907   0   0   0   0   1   0   0   0  ...     0    0    0    1   \n",
      "35     1      783   0   0   0   0   1   0   0   0  ...     0    1    0    1   \n",
      "36     1     1836   0   0   0   0   0   0   0   1  ...     0    0    0    0   \n",
      "37     1     1022   0   0   0   0   1   0   0   0  ...     0    1    0    1   \n",
      "38     1     2762   0   0   0   0   0   0   0   0  ...     0    0    0    0   \n",
      "39     1      150   0   0   0   0   0   0   0   1  ...     0    0    0    0   \n",
      "40     1        1   0   0   1   0   1   0   0   0  ...     0    0    0    1   \n",
      "41     1     1961   0   0   0   0   0   0   0   1  ...     0    0    0    0   \n",
      "42     1     1962   0   0   0   0   0   0   0   1  ...     0    0    0    0   \n",
      "43     1     2692   1   0   0   0   0   1   0   0  ...     0    0    0    0   \n",
      "44     1      260   1   1   0   0   0   0   0   0  ...     0    0    0    0   \n",
      "45     1     1028   0   0   1   0   1   0   0   0  ...     0    1    0    0   \n",
      "46     1     1029   0   0   0   0   1   0   0   0  ...     0    1    0    1   \n",
      "47     1     1207   0   0   0   0   0   0   0   1  ...     0    0    0    0   \n",
      "48     1     2028   1   0   0   0   0   0   0   1  ...     0    0    0    0   \n",
      "49     1      531   0   0   0   0   1   0   0   1  ...     0    0    0    0   \n",
      "50     1     3114   0   0   1   0   1   0   0   0  ...     0    0    0    1   \n",
      "51     1      608   0   0   0   0   0   1   0   1  ...     0    0    0    0   \n",
      "52     1     1246   0   0   0   0   0   0   0   1  ...     0    0    0    0   \n",
      "\n",
      "    a14  a15  a16  a17  a18  like  \n",
      "0     0    0    0    0    0     1  \n",
      "1     0    0    0    0    0     0  \n",
      "2     1    0    0    0    0     0  \n",
      "3     0    0    0    0    0     1  \n",
      "4     0    0    0    0    0     1  \n",
      "5     1    0    0    0    0     0  \n",
      "6     0    0    0    0    0     1  \n",
      "7     0    0    0    0    0     1  \n",
      "8     0    0    0    0    0     1  \n",
      "9     0    0    0    0    0     1  \n",
      "10    0    0    0    0    0     1  \n",
      "11    0    0    0    0    0     1  \n",
      "12    0    0    0    0    0     1  \n",
      "13    0    0    0    0    0     1  \n",
      "14    0    0    0    0    0     1  \n",
      "15    0    0    0    0    0     1  \n",
      "16    0    0    0    0    0     0  \n",
      "17    0    0    0    0    0     1  \n",
      "18    0    0    0    0    0     1  \n",
      "19    0    0    0    0    0     1  \n",
      "20    0    0    0    0    0     0  \n",
      "21    0    0    0    0    0     0  \n",
      "22    0    1    0    0    0     1  \n",
      "23    0    0    0    1    0     1  \n",
      "24    1    0    0    0    0     0  \n",
      "25    1    0    0    0    0     1  \n",
      "26    0    1    0    0    0     1  \n",
      "27    1    0    0    0    0     1  \n",
      "28    0    0    0    0    0     1  \n",
      "29    0    0    1    0    0     0  \n",
      "30    0    0    0    0    0     1  \n",
      "31    0    0    0    0    0     1  \n",
      "32    0    0    0    0    0     1  \n",
      "33    0    0    0    0    0     1  \n",
      "34    0    0    0    0    0     1  \n",
      "35    0    0    0    0    0     1  \n",
      "36    0    0    0    0    0     1  \n",
      "37    0    0    0    0    0     1  \n",
      "38    0    0    1    0    0     1  \n",
      "39    0    0    0    0    0     1  \n",
      "40    0    0    0    0    0     1  \n",
      "41    0    0    0    0    0     1  \n",
      "42    0    0    0    0    0     1  \n",
      "43    1    0    0    0    0     1  \n",
      "44    0    1    0    0    0     1  \n",
      "45    0    0    0    0    0     1  \n",
      "46    0    0    0    0    0     1  \n",
      "47    0    0    0    0    0     1  \n",
      "48    0    0    0    1    0     1  \n",
      "49    0    0    0    0    0     1  \n",
      "50    0    0    0    0    0     1  \n",
      "51    0    0    1    0    0     1  \n",
      "52    0    0    0    0    0     1  \n",
      "\n",
      "[53 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import Layer, Dense, Dropout, LSTMCell, LSTM, GRU, RNN, Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from random import shuffle\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/siddharth/Documents/progs/ml-1m/ml-1m/user1_ratings.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 19\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "df1 = df[['a1','a2','a3','a4','a5','a6','a7','a8','a9','a10','a11','a12','a13','a14','a15','a16','a17','a18']]\n",
    "df2 = df[['like']]\n",
    "dataset = df1.values\n",
    "dataset = dataset.astype('float32')\n",
    "dataset2 = df2.values\n",
    "dataset2 = dataset2.astype('float32')\n",
    "train_size = int(len(dataset) * 0.65)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size], dataset[train_size:len(dataset)]\n",
    "train_op, test_op = dataset2[0:train_size], dataset2[train_size:len(dataset2)]\n",
    "print(len(train), len(test))\n",
    "print(dataset)\n",
    "print(train)\n",
    "print(test)\n",
    "print(train_op)\n",
    "print(test_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "Compiling...\n",
      "Epoch 1/100\n",
      "34/34 [==============================] - 3s 84ms/step - loss: 1.0278 - acc: 0.2353\n",
      "Epoch 2/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.7927 - acc: 0.2353\n",
      "Epoch 3/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6318 - acc: 0.7647\n",
      "Epoch 4/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5524 - acc: 0.7647\n",
      "Epoch 5/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5502 - acc: 0.7647\n",
      "Epoch 6/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5523 - acc: 0.7647\n",
      "Epoch 7/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5635 - acc: 0.7647\n",
      "Epoch 8/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5620 - acc: 0.7647\n",
      "Epoch 9/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5570 - acc: 0.7647\n",
      "Epoch 10/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5506 - acc: 0.7647\n",
      "Epoch 11/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5485 - acc: 0.7647\n",
      "Epoch 12/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5457 - acc: 0.7647\n",
      "Epoch 13/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5503 - acc: 0.7647\n",
      "Epoch 14/100\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.5467 - acc: 0.7647\n",
      "Epoch 15/100\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.5464 - acc: 0.7647\n",
      "Epoch 16/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5459 - acc: 0.7647\n",
      "Epoch 17/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5468 - acc: 0.7647\n",
      "Epoch 18/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5467 - acc: 0.7647\n",
      "Epoch 19/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5460 - acc: 0.7647\n",
      "Epoch 20/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5472 - acc: 0.7647\n",
      "Epoch 21/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5476 - acc: 0.7647\n",
      "Epoch 22/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5454 - acc: 0.7647\n",
      "Epoch 23/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5449 - acc: 0.7647\n",
      "Epoch 24/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5451 - acc: 0.7647\n",
      "Epoch 25/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5473 - acc: 0.7647\n",
      "Epoch 26/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5474 - acc: 0.7647\n",
      "Epoch 27/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5451 - acc: 0.7647\n",
      "Epoch 28/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5468 - acc: 0.7647\n",
      "Epoch 29/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5459 - acc: 0.7647\n",
      "Epoch 30/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5462 - acc: 0.7647\n",
      "Epoch 31/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5456 - acc: 0.7647\n",
      "Epoch 32/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5454 - acc: 0.7647\n",
      "Epoch 33/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5495 - acc: 0.7647\n",
      "Epoch 34/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5490 - acc: 0.7647\n",
      "Epoch 35/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5481 - acc: 0.7647\n",
      "Epoch 36/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5457 - acc: 0.7647\n",
      "Epoch 37/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5452 - acc: 0.7647\n",
      "Epoch 38/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5451 - acc: 0.7647\n",
      "Epoch 39/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5447 - acc: 0.7647\n",
      "Epoch 40/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5460 - acc: 0.7647\n",
      "Epoch 41/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5466 - acc: 0.7647\n",
      "Epoch 42/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5484 - acc: 0.7647\n",
      "Epoch 43/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5477 - acc: 0.7647\n",
      "Epoch 44/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5468 - acc: 0.7647\n",
      "Epoch 45/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5482 - acc: 0.7647\n",
      "Epoch 46/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5473 - acc: 0.7647\n",
      "Epoch 47/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5477 - acc: 0.7647\n",
      "Epoch 48/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5534 - acc: 0.7647\n",
      "Epoch 49/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5466 - acc: 0.7647\n",
      "Epoch 50/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5447 - acc: 0.7647\n",
      "Epoch 51/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5454 - acc: 0.7647\n",
      "Epoch 52/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5484 - acc: 0.7647\n",
      "Epoch 53/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5467 - acc: 0.7647\n",
      "Epoch 54/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5458 - acc: 0.7647\n",
      "Epoch 55/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5449 - acc: 0.7647\n",
      "Epoch 56/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5451 - acc: 0.7647\n",
      "Epoch 57/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5464 - acc: 0.7647\n",
      "Epoch 58/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5443 - acc: 0.7647\n",
      "Epoch 59/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5457 - acc: 0.7647\n",
      "Epoch 60/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5476 - acc: 0.7647\n",
      "Epoch 61/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5512 - acc: 0.7647\n",
      "Epoch 62/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5449 - acc: 0.7647\n",
      "Epoch 63/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5463 - acc: 0.7647\n",
      "Epoch 64/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5452 - acc: 0.7647\n",
      "Epoch 65/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5450 - acc: 0.7647\n",
      "Epoch 66/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5459 - acc: 0.7647\n",
      "Epoch 67/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5458 - acc: 0.7647\n",
      "Epoch 68/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5455 - acc: 0.7647\n",
      "Epoch 69/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5460 - acc: 0.7647\n",
      "Epoch 70/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5460 - acc: 0.7647\n",
      "Epoch 71/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5509 - acc: 0.7647\n",
      "Epoch 72/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5460 - acc: 0.7647\n",
      "Epoch 73/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5456 - acc: 0.7647\n",
      "Epoch 74/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5427 - acc: 0.7647\n",
      "Epoch 75/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5434 - acc: 0.7647\n",
      "Epoch 76/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5448 - acc: 0.7647\n",
      "Epoch 77/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5450 - acc: 0.7647\n",
      "Epoch 78/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5467 - acc: 0.7647\n",
      "Epoch 79/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5477 - acc: 0.7647\n",
      "Epoch 80/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5501 - acc: 0.7647\n",
      "Epoch 81/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5451 - acc: 0.7647\n",
      "Epoch 82/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5446 - acc: 0.7647\n",
      "Epoch 83/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5451 - acc: 0.7647\n",
      "Epoch 84/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5417 - acc: 0.7647\n",
      "Epoch 85/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5506 - acc: 0.7647\n",
      "Epoch 86/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5481 - acc: 0.7647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5473 - acc: 0.7647\n",
      "Epoch 88/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5487 - acc: 0.7647\n",
      "Epoch 89/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5406 - acc: 0.7647\n",
      "Epoch 90/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5460 - acc: 0.7647\n",
      "Epoch 91/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5501 - acc: 0.7647\n",
      "Epoch 92/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5516 - acc: 0.7647\n",
      "Epoch 93/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5529 - acc: 0.7647\n",
      "Epoch 94/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5472 - acc: 0.7647\n",
      "Epoch 95/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5470 - acc: 0.7647\n",
      "Epoch 96/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5485 - acc: 0.7647\n",
      "Epoch 97/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5413 - acc: 0.7647\n",
      "Epoch 98/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5416 - acc: 0.7647\n",
      "Epoch 99/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5429 - acc: 0.7647\n",
      "Epoch 100/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5424 - acc: 0.7647\n",
      "Test accuracy : 84.91%\n"
     ]
    }
   ],
   "source": [
    "def create_model(input_length):\n",
    "    print ('Creating model...')\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = 200, output_dim = 50, input_length = input_length))\n",
    "    model.add(LSTM(activation='sigmoid',units = 100, recurrent_activation='sigmoid'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    print ('Compiling...')\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model(len(train[0]))\n",
    "hist = model.fit(train, train_op, epochs=100,batch_size=10)\n",
    "\n",
    "scores = model.evaluate(dataset, dataset2, verbose=0)\n",
    "print(\"Test accuracy : %.2f%%\" %((scores[1]*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import activations\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "\n",
    "class ModifiedLSTMCell(LSTMCell):\n",
    "\n",
    "    def __init__(self, units,\n",
    "                 activation='tanh',\n",
    "                 recurrent_activation='sigmoid',\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 unit_forget_bias=True,\n",
    "                 kernel_regularizer=None,\n",
    "                 recurrent_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 recurrent_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 dropout=0.,\n",
    "                 recurrent_dropout=0.,\n",
    "                 implementation=1,\n",
    "                 **kwargs):\n",
    "        super(LSTMCell, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.recurrent_activation = activations.get(recurrent_activation)\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.unit_forget_bias = unit_forget_bias\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        self.dropout = min(1., max(0., dropout))\n",
    "        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
    "        self.implementation = implementation\n",
    "        self.state_size = (self.units, self.units)\n",
    "        self.output_size = self.units\n",
    "        self._dropout_mask = None\n",
    "        self._recurrent_dropout_mask = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        self.kernel = self.add_weight(shape=(input_dim, self.units * 4),\n",
    "                                      name='kernel',\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            shape=(self.units, self.units * 4),\n",
    "            name='recurrent_kernel',\n",
    "            initializer=self.recurrent_initializer,\n",
    "            regularizer=self.recurrent_regularizer,\n",
    "            constraint=self.recurrent_constraint)\n",
    "\n",
    "        if self.use_bias:\n",
    "            if self.unit_forget_bias:\n",
    "                def bias_initializer(_, *args, **kwargs):\n",
    "                    return K.concatenate([\n",
    "                        self.bias_initializer((self.units,), *args, **kwargs),\n",
    "                        initializers.Ones()((self.units,), *args, **kwargs),\n",
    "                        self.bias_initializer((self.units * 2,), *args, **kwargs),\n",
    "                    ])\n",
    "            else:\n",
    "                bias_initializer = self.bias_initializer\n",
    "            self.bias = self.add_weight(shape=(self.units * 4,),\n",
    "                                        name='bias',\n",
    "                                        initializer=bias_initializer,\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        self.kernel_i = self.kernel[:, :self.units]\n",
    "        self.kernel_f = self.kernel[:, self.units: self.units * 2]\n",
    "        self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]\n",
    "        self.kernel_o = self.kernel[:, self.units * 3:]\n",
    "\n",
    "        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n",
    "        self.recurrent_kernel_f = (\n",
    "            self.recurrent_kernel[:, self.units: self.units * 2])\n",
    "        self.recurrent_kernel_c = (\n",
    "            self.recurrent_kernel[:, self.units * 2: self.units * 3])\n",
    "        self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias_i = self.bias[:self.units]\n",
    "            self.bias_f = self.bias[self.units: self.units * 2]\n",
    "            self.bias_c = self.bias[self.units * 2: self.units * 3]\n",
    "            self.bias_o = self.bias[self.units * 3:]\n",
    "        else:\n",
    "            self.bias_i = None\n",
    "            self.bias_f = None\n",
    "            self.bias_c = None\n",
    "            self.bias_o = None\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states, training=None):\n",
    "        if 0 < self.dropout < 1 and self._dropout_mask is None:\n",
    "            self._dropout_mask = _generate_dropout_mask(\n",
    "                K.ones_like(inputs),\n",
    "                self.dropout,\n",
    "                training=training,\n",
    "                count=4)\n",
    "        if (0 < self.recurrent_dropout < 1 and\n",
    "                self._recurrent_dropout_mask is None):\n",
    "            self._recurrent_dropout_mask = _generate_dropout_mask(\n",
    "                K.ones_like(states[0]),\n",
    "                self.recurrent_dropout,\n",
    "                training=training,\n",
    "                count=4)\n",
    "\n",
    "        # dropout matrices for input units\n",
    "        dp_mask = self._dropout_mask\n",
    "        # dropout matrices for recurrent units\n",
    "        rec_dp_mask = self._recurrent_dropout_mask\n",
    "\n",
    "        h_tm1 = states[0]  # previous memory state\n",
    "        c_tm1 = states[1]  # previous carry state\n",
    "\n",
    "        if self.implementation == 1:\n",
    "            if 0 < self.dropout < 1.:\n",
    "                inputs_i = inputs * dp_mask[0]\n",
    "                inputs_f = inputs * dp_mask[1]\n",
    "                inputs_c = inputs * dp_mask[2]\n",
    "                inputs_o = inputs * dp_mask[3]\n",
    "            else:\n",
    "                inputs_i = inputs\n",
    "                inputs_f = inputs\n",
    "                inputs_c = inputs\n",
    "                inputs_o = inputs\n",
    "            x_i = K.dot(inputs_i, self.kernel_i)\n",
    "            x_f = K.dot(inputs_f, self.kernel_f)\n",
    "            x_c = K.dot(inputs_c, self.kernel_c)\n",
    "            x_o = K.dot(inputs_o, self.kernel_o)\n",
    "            if self.use_bias:\n",
    "                x_i = K.bias_add(x_i, self.bias_i)\n",
    "                x_f = K.bias_add(x_f, self.bias_f)\n",
    "                x_c = K.bias_add(x_c, self.bias_c)\n",
    "                x_o = K.bias_add(x_o, self.bias_o)\n",
    "\n",
    "            if 0 < self.recurrent_dropout < 1.:\n",
    "                h_tm1_i = h_tm1 * rec_dp_mask[0]\n",
    "                h_tm1_f = h_tm1 * rec_dp_mask[1]\n",
    "                h_tm1_c = h_tm1 * rec_dp_mask[2]\n",
    "                h_tm1_o = h_tm1 * rec_dp_mask[3]\n",
    "            else:\n",
    "                h_tm1_i = h_tm1\n",
    "                h_tm1_f = h_tm1\n",
    "                h_tm1_c = h_tm1\n",
    "                h_tm1_o = h_tm1\n",
    "            i = self.recurrent_activation(x_i + K.dot(h_tm1_i,\n",
    "                                                      self.recurrent_kernel_i))\n",
    "            f = 0\n",
    "            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1_c,\n",
    "                                                            self.recurrent_kernel_c))\n",
    "            o = self.recurrent_activation(x_o + K.dot(h_tm1_o,\n",
    "                                                      self.recurrent_kernel_o))\n",
    "        else:\n",
    "            if 0. < self.dropout < 1.:\n",
    "                inputs *= dp_mask[0]\n",
    "            z = K.dot(inputs, self.kernel)\n",
    "            if 0. < self.recurrent_dropout < 1.:\n",
    "                h_tm1 *= rec_dp_mask[0]\n",
    "            z += K.dot(h_tm1, self.recurrent_kernel)\n",
    "            if self.use_bias:\n",
    "                z = K.bias_add(z, self.bias)\n",
    "\n",
    "            z0 = z[:, :self.units]\n",
    "            z1 = z[:, self.units: 2 * self.units]\n",
    "            z2 = z[:, 2 * self.units: 3 * self.units]\n",
    "            z3 = z[:, 3 * self.units:]\n",
    "\n",
    "            i = self.recurrent_activation(z0)\n",
    "            f = self.recurrent_activation(z1)\n",
    "            c = f * c_tm1 + i * self.activation(z2)\n",
    "            o = self.recurrent_activation(z3)\n",
    "\n",
    "        h = o * self.activation(c)\n",
    "        if 0 < self.dropout + self.recurrent_dropout:\n",
    "            if training is None:\n",
    "                h._uses_learning_phase = True\n",
    "        return h, [h, c]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'units': self.units,\n",
    "                  'activation': activations.serialize(self.activation),\n",
    "                  'recurrent_activation':\n",
    "                      activations.serialize(self.recurrent_activation),\n",
    "                  'use_bias': self.use_bias,\n",
    "                  'kernel_initializer':\n",
    "                      initializers.serialize(self.kernel_initializer),\n",
    "                  'recurrent_initializer':\n",
    "                      initializers.serialize(self.recurrent_initializer),\n",
    "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "                  'unit_forget_bias': self.unit_forget_bias,\n",
    "                  'kernel_regularizer':\n",
    "                      regularizers.serialize(self.kernel_regularizer),\n",
    "                  'recurrent_regularizer':\n",
    "                      regularizers.serialize(self.recurrent_regularizer),\n",
    "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
    "                  'recurrent_constraint':\n",
    "                      constraints.serialize(self.recurrent_constraint),\n",
    "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
    "                  'dropout': self.dropout,\n",
    "                  'recurrent_dropout': self.recurrent_dropout,\n",
    "                  'implementation': self.implementation}\n",
    "        base_config = super(LSTMCell, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedLSTM(LSTM):\n",
    "\n",
    "    def __init__(self, units,\n",
    "                 activation='tanh',\n",
    "                 recurrent_activation='sigmoid',\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 unit_forget_bias=True,\n",
    "                 kernel_regularizer=None,\n",
    "                 recurrent_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 recurrent_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 dropout=0.,\n",
    "                 recurrent_dropout=0.,\n",
    "                 implementation=1,\n",
    "                 return_sequences=False,\n",
    "                 return_state=False,\n",
    "                 go_backwards=False,\n",
    "                 stateful=False,\n",
    "                 unroll=False,\n",
    "                 **kwargs):\n",
    "        if implementation == 0:\n",
    "            warnings.warn('`implementation=0` has been deprecated, '\n",
    "                          'and now defaults to `implementation=1`.'\n",
    "                          'Please update your layer call.')\n",
    "        if K.backend() == 'theano' and (dropout or recurrent_dropout):\n",
    "            warnings.warn(\n",
    "                'RNN dropout is no longer supported with the Theano backend '\n",
    "                'due to technical limitations. '\n",
    "                'You can either set `dropout` and `recurrent_dropout` to 0, '\n",
    "                'or use the TensorFlow backend.')\n",
    "            dropout = 0.\n",
    "            recurrent_dropout = 0.\n",
    "\n",
    "        cell = ModifiedLSTMCell(units,\n",
    "                        activation=activation,\n",
    "                        recurrent_activation=recurrent_activation,\n",
    "                        use_bias=use_bias,\n",
    "                        kernel_initializer=kernel_initializer,\n",
    "                        recurrent_initializer=recurrent_initializer,\n",
    "                        unit_forget_bias=unit_forget_bias,\n",
    "                        bias_initializer=bias_initializer,\n",
    "                        kernel_regularizer=kernel_regularizer,\n",
    "                        recurrent_regularizer=recurrent_regularizer,\n",
    "                        bias_regularizer=bias_regularizer,\n",
    "                        kernel_constraint=kernel_constraint,\n",
    "                        recurrent_constraint=recurrent_constraint,\n",
    "                        bias_constraint=bias_constraint,\n",
    "                        dropout=dropout,\n",
    "                        recurrent_dropout=recurrent_dropout,\n",
    "                        implementation=implementation)\n",
    "        super(LSTM, self).__init__(cell,\n",
    "                                   return_sequences=return_sequences,\n",
    "                                   return_state=return_state,\n",
    "                                   go_backwards=go_backwards,\n",
    "                                   stateful=stateful,\n",
    "                                   unroll=unroll,\n",
    "                                   **kwargs)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "    def call(self, inputs, mask=None, training=None, initial_state=None):\n",
    "        self.cell._dropout_mask = None\n",
    "        self.cell._recurrent_dropout_mask = None\n",
    "        return super(LSTM, self).call(inputs,\n",
    "                                      mask=mask,\n",
    "                                      training=training,\n",
    "                                      initial_state=initial_state)\n",
    "\n",
    "    @property\n",
    "    def units(self):\n",
    "        return self.cell.units\n",
    "\n",
    "    @property\n",
    "    def activation(self):\n",
    "        return self.cell.activation\n",
    "\n",
    "    @property\n",
    "    def recurrent_activation(self):\n",
    "        return self.cell.recurrent_activation\n",
    "\n",
    "    @property\n",
    "    def use_bias(self):\n",
    "        return self.cell.use_bias\n",
    "\n",
    "    @property\n",
    "    def kernel_initializer(self):\n",
    "        return self.cell.kernel_initializer\n",
    "\n",
    "    @property\n",
    "    def recurrent_initializer(self):\n",
    "        return self.cell.recurrent_initializer\n",
    "\n",
    "    @property\n",
    "    def bias_initializer(self):\n",
    "        return self.cell.bias_initializer\n",
    "\n",
    "    @property\n",
    "    def unit_forget_bias(self):\n",
    "        return self.cell.unit_forget_bias\n",
    "\n",
    "    @property\n",
    "    def kernel_regularizer(self):\n",
    "        return self.cell.kernel_regularizer\n",
    "\n",
    "    @property\n",
    "    def recurrent_regularizer(self):\n",
    "        return self.cell.recurrent_regularizer\n",
    "\n",
    "    @property\n",
    "    def bias_regularizer(self):\n",
    "        return self.cell.bias_regularizer\n",
    "\n",
    "    @property\n",
    "    def kernel_constraint(self):\n",
    "        return self.cell.kernel_constraint\n",
    "\n",
    "    @property\n",
    "    def recurrent_constraint(self):\n",
    "        return self.cell.recurrent_constraint\n",
    "\n",
    "    @property\n",
    "    def bias_constraint(self):\n",
    "        return self.cell.bias_constraint\n",
    "\n",
    "    @property\n",
    "    def dropout(self):\n",
    "        return self.cell.dropout\n",
    "\n",
    "    @property\n",
    "    def recurrent_dropout(self):\n",
    "        return self.cell.recurrent_dropout\n",
    "\n",
    "    @property\n",
    "    def implementation(self):\n",
    "        return self.cell.implementation\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'units': self.units,\n",
    "                  'activation': activations.serialize(self.activation),\n",
    "                  'recurrent_activation':\n",
    "                      activations.serialize(self.recurrent_activation),\n",
    "                  'use_bias': self.use_bias,\n",
    "                  'kernel_initializer':\n",
    "                      initializers.serialize(self.kernel_initializer),\n",
    "                  'recurrent_initializer':\n",
    "                      initializers.serialize(self.recurrent_initializer),\n",
    "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "                  'unit_forget_bias': self.unit_forget_bias,\n",
    "                  'kernel_regularizer':\n",
    "                      regularizers.serialize(self.kernel_regularizer),\n",
    "                  'recurrent_regularizer':\n",
    "                      regularizers.serialize(self.recurrent_regularizer),\n",
    "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "                  'activity_regularizer':\n",
    "                      regularizers.serialize(self.activity_regularizer),\n",
    "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
    "                  'recurrent_constraint':\n",
    "                      constraints.serialize(self.recurrent_constraint),\n",
    "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
    "                  'dropout': self.dropout,\n",
    "                  'recurrent_dropout': self.recurrent_dropout,\n",
    "                  'implementation': self.implementation}\n",
    "        base_config = super(LSTM, self).get_config()\n",
    "        del base_config['cell']\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        if 'implementation' in config and config['implementation'] == 0:\n",
    "            config['implementation'] = 1\n",
    "        return cls(**config)\n",
    "\n",
    "\n",
    "def _generate_dropout_mask(ones, rate, training=None, count=1):\n",
    "    def dropped_inputs():\n",
    "        return K.dropout(ones, rate)\n",
    "\n",
    "    if count > 1:\n",
    "        return [K.in_train_phase(\n",
    "            dropped_inputs,\n",
    "            ones,\n",
    "            training=training) for _ in range(count)]\n",
    "    return K.in_train_phase(\n",
    "        dropped_inputs,\n",
    "        ones,\n",
    "        training=training)\n",
    "\n",
    "\n",
    "def _standardize_args(inputs, initial_state, constants, num_constants):\n",
    "    \n",
    "    if isinstance(inputs, list):\n",
    "        assert initial_state is None and constants is None\n",
    "        if num_constants is not None:\n",
    "            constants = inputs[-num_constants:]\n",
    "            inputs = inputs[:-num_constants]\n",
    "        if len(inputs) > 1:\n",
    "            initial_state = inputs[1:]\n",
    "        inputs = inputs[0]\n",
    "\n",
    "    def to_list_or_none(x):\n",
    "        if x is None or isinstance(x, list):\n",
    "            return x\n",
    "        if isinstance(x, tuple):\n",
    "            return list(x)\n",
    "        return [x]\n",
    "\n",
    "    initial_state = to_list_or_none(initial_state)\n",
    "    constants = to_list_or_none(constants)\n",
    "\n",
    "    return inputs, initial_state, constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "Compiling...\n",
      "Epoch 1/100\n",
      "34/34 [==============================] - 1s 41ms/step - loss: 0.6885 - acc: 0.6471\n",
      "Epoch 2/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6803 - acc: 0.7353\n",
      "Epoch 3/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6736 - acc: 0.7647\n",
      "Epoch 4/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6709 - acc: 0.7647\n",
      "Epoch 5/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6652 - acc: 0.7647\n",
      "Epoch 6/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6541 - acc: 0.7647\n",
      "Epoch 7/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6483 - acc: 0.7647\n",
      "Epoch 8/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6427 - acc: 0.7647\n",
      "Epoch 9/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6251 - acc: 0.7647\n",
      "Epoch 10/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6115 - acc: 0.7647\n",
      "Epoch 11/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6080 - acc: 0.7647\n",
      "Epoch 12/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5882 - acc: 0.7647\n",
      "Epoch 13/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5936 - acc: 0.7647\n",
      "Epoch 14/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5790 - acc: 0.7647\n",
      "Epoch 15/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5815 - acc: 0.7647\n",
      "Epoch 16/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5949 - acc: 0.7647\n",
      "Epoch 17/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5511 - acc: 0.7647\n",
      "Epoch 18/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.5412 - acc: 0.7647\n",
      "Epoch 19/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5449 - acc: 0.7647\n",
      "Epoch 20/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5834 - acc: 0.7647\n",
      "Epoch 21/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5698 - acc: 0.7647\n",
      "Epoch 22/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5662 - acc: 0.7647\n",
      "Epoch 23/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5167 - acc: 0.7647\n",
      "Epoch 24/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5684 - acc: 0.7647\n",
      "Epoch 25/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5693 - acc: 0.7647\n",
      "Epoch 26/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5662 - acc: 0.7647\n",
      "Epoch 27/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5864 - acc: 0.7647\n",
      "Epoch 28/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5493 - acc: 0.7647\n",
      "Epoch 29/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5917 - acc: 0.7647\n",
      "Epoch 30/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5396 - acc: 0.7647\n",
      "Epoch 31/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5799 - acc: 0.7647\n",
      "Epoch 32/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5641 - acc: 0.7647\n",
      "Epoch 33/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5977 - acc: 0.7647\n",
      "Epoch 34/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5777 - acc: 0.7647\n",
      "Epoch 35/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5589 - acc: 0.7647\n",
      "Epoch 36/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5245 - acc: 0.7647\n",
      "Epoch 37/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5472 - acc: 0.7647\n",
      "Epoch 38/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5930 - acc: 0.7647\n",
      "Epoch 39/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5699 - acc: 0.7647\n",
      "Epoch 40/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5471 - acc: 0.7647\n",
      "Epoch 41/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5389 - acc: 0.7647\n",
      "Epoch 42/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5797 - acc: 0.7647\n",
      "Epoch 43/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5960 - acc: 0.7647\n",
      "Epoch 44/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5287 - acc: 0.7647\n",
      "Epoch 45/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5719 - acc: 0.7647\n",
      "Epoch 46/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5393 - acc: 0.7647\n",
      "Epoch 47/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5757 - acc: 0.7647A: 0s - loss: 0.6127 - acc: 0.750\n",
      "Epoch 48/100\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.5647 - acc: 0.7647\n",
      "Epoch 49/100\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.5719 - acc: 0.7647\n",
      "Epoch 50/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5614 - acc: 0.7647\n",
      "Epoch 51/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5467 - acc: 0.7647\n",
      "Epoch 52/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5252 - acc: 0.7647\n",
      "Epoch 53/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5651 - acc: 0.7647\n",
      "Epoch 54/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5901 - acc: 0.7647\n",
      "Epoch 55/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5164 - acc: 0.7647\n",
      "Epoch 56/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5428 - acc: 0.7647\n",
      "Epoch 57/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5529 - acc: 0.7647\n",
      "Epoch 58/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5474 - acc: 0.7647\n",
      "Epoch 59/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5507 - acc: 0.7647\n",
      "Epoch 60/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5621 - acc: 0.7647\n",
      "Epoch 61/100\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5903 - acc: 0.7647\n",
      "Epoch 62/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5332 - acc: 0.7647\n",
      "Epoch 63/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.5591 - acc: 0.7647\n",
      "Epoch 64/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5565 - acc: 0.7647\n",
      "Epoch 65/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5896 - acc: 0.7647\n",
      "Epoch 66/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5591 - acc: 0.7647\n",
      "Epoch 67/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.4962 - acc: 0.7647\n",
      "Epoch 68/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5431 - acc: 0.7647\n",
      "Epoch 69/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5606 - acc: 0.7647\n",
      "Epoch 70/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5921 - acc: 0.7647\n",
      "Epoch 71/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5321 - acc: 0.7647\n",
      "Epoch 72/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6157 - acc: 0.7647\n",
      "Epoch 73/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5600 - acc: 0.7647\n",
      "Epoch 74/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5621 - acc: 0.7647\n",
      "Epoch 75/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5608 - acc: 0.7647\n",
      "Epoch 76/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5495 - acc: 0.7647\n",
      "Epoch 77/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5563 - acc: 0.7647\n",
      "Epoch 78/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5671 - acc: 0.7647\n",
      "Epoch 79/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6035 - acc: 0.7647\n",
      "Epoch 80/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6269 - acc: 0.7647\n",
      "Epoch 81/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5634 - acc: 0.7647\n",
      "Epoch 82/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5834 - acc: 0.7647\n",
      "Epoch 83/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5392 - acc: 0.7647\n",
      "Epoch 84/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6064 - acc: 0.7647\n",
      "Epoch 85/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5253 - acc: 0.7647\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5605 - acc: 0.7647\n",
      "Epoch 87/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.5879 - acc: 0.7647\n",
      "Epoch 88/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.5501 - acc: 0.7647\n",
      "Epoch 89/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5623 - acc: 0.7647\n",
      "Epoch 90/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.5497 - acc: 0.7647\n",
      "Epoch 91/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.5706 - acc: 0.7647\n",
      "Epoch 92/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.5862 - acc: 0.7647\n",
      "Epoch 93/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.5523 - acc: 0.7647\n",
      "Epoch 94/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.4902 - acc: 0.7647\n",
      "Epoch 95/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.5277 - acc: 0.7647\n",
      "Epoch 96/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.5062 - acc: 0.7647\n",
      "Epoch 97/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5346 - acc: 0.7647\n",
      "Epoch 98/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5327 - acc: 0.7647\n",
      "Epoch 99/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.4989 - acc: 0.7647\n",
      "Epoch 100/100\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6075 - acc: 0.7647\n",
      "Test accuracy : 86.91%\n"
     ]
    }
   ],
   "source": [
    "def create_my_model(input_length):\n",
    "    print ('Creating model...')\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = 100, output_dim = 50, input_length = input_length))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(ModifiedLSTM(activation='sigmoid',units = input_length, recurrent_activation='tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='hard_sigmoid'))\n",
    "\n",
    "    print ('Compiling...')\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_my_model(len(train[0]))\n",
    "hist = model.fit(train, train_op, epochs=100,batch_size=10)\n",
    "\n",
    "scores = model.evaluate(dataset, dataset2, verbose=0)\n",
    "print(\"Test accuracy : %.2f%%\" %((scores[1]*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
